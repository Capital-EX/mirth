
import base/panic
import base/int
import base/nat
import base/pos
import base/char
import base/order
import base/list
import base/str
import base/regex+
import base/maybe
import base/result
import mirth/types/mod
import mirth/loc
import mirth/token

export mirth/lexer
  type LToken
  type LexerError
  lexerError->str : LexerError -- Str
  tokenize : Mod Str -- Result(LexerError, List(LToken))
end


#
# auxiliary lexing functions
#

digitVal : Char -- Int
digitVal =
  cond(
    isdigit? -> char->int "0" str->char char->int z-,
    islower? -> char->int "a" str->char char->int z- 10 z+,
    isupper? -> char->int "A" str->char char->int z- 10 z+,
    drop 0
  )

readInt(base: Int) : Str -- Int
readInt(base) = dip(0) strfold(dip(base z*) digitVal z+)

readDecInt : Str -- Int
readHexInt : Str -- Int
readDecInt = readInt(10)
readHexInt = n2 strbreak nip readInt(16)

readStr    : Str -- Str
wordToken  : Str -- Token  # basically TWORD but separates out reserved words.

#
# regexes
#

decIntRegex : Regex
decIntRegex = "" lit "+-" cls alt2 "0123456789" cls plus seq2

hexIntRegex : Regex
hexIntRegex = "" lit "+-" cls alt2 "0" lit "xX" cls "0123456789abcdefABCDEF" cls plus seq4

#
#
#

data LexerRule
  lexerRule(f: Str -- Token) : Regex -- LexerRule
end


lexerRules : List(LexerRule)
lexerRules = $(
  nil

  # ints
  decIntRegex lexerRule(readDecInt TINT) cons
  hexIntRegex lexerRule(readHexInt TINT) cons

  # strings
  # TODO

  # words -- we use wordToken to disambiguate between reserved words and regular words.
  # TODO
)

data LexerMatch
  lexerMatch(f: Str -- Token) : Maybe(Nat) -- LexerMatch
end

lexerMatchUnwrap : LexerMatch -- Maybe(Nat)
lexerMatchUnwrap = match( lexerMatch(f) -> id )

lexerMatchCmp? : LexerMatch LexerMatch -- LexerMatch LexerMatch Comp
lexerMatchCmp? = dup2 both(lexerMatchUnwrap) mcmp(ncmp)

lexerMatchOrder : Order(LexerMatch)
lexerMatchOrder = MkOrder(lexerMatchCmp?)

bestLexerMatch : Str -- LexerMatch
bestLexerMatch =
  lexerRules formap(
    match( lexerRule(f) ->
      dip(dup) regexMatch lexerMatch(f)
    )
  ) nip maximum(lexerMatchOrder)

data LexerError
  lexerError : Loc Str -- LexerError
end

unLexerError : LexerError -- Loc Str
unLexerError = match( lexerError -> id )

lexerError->str : LexerError -- Str
lexerError->str = unLexerError dip(loc->str ":" <>) <>

data LToken
  ltoken : Loc Loc Token -- LToken
end

unltoken : LToken -- Loc Loc Token
unltoken = match( ltoken -> id )

tokenizeAux : List(LToken) Loc Str -- Result(LexerError, List(LToken))
tokenizeAux =
  cond(
    strnull? -> drop2 ok,
    dup bestLexerMatch match( lexerMatch(f) ->
      maybe(
        drop nip "Unrecognized token." lexerError err,
        strbreak dip(
          dip(dup) dup
          dip(locNext) f over
          dip(ltoken cons)
        ) tokenizeAux
      )
    )
  )

tokenize : Mod Str -- Result(LexerError, List(LToken))
tokenize = dip2(nil) dip(locStart) tokenizeAux



"10" tokenize
 == lambda(m -> nil m p1 p1 loc m p1 3 zpos loc 10 TINT ltoken cons ok)


