
import base/panic
import base/int
import base/nat
import base/pos
import base/char
import base/order
import base/list
import base/str
import base/regex+
import base/maybe
import base/result
import mirth/types/mod
import mirth/loc
import mirth/token

export mirth/lexer
  type LToken
  type LexerError
  lexerError->str : LexerError -- Str
  tokenize : Mod Str -- Result(LexerError, List(LToken))
end


#
# auxiliary lexing functions
#

digitVal : Char -- Int
digitVal =
  cond(
    isdigit? -> char->int "0" str->char char->int z-,
    islower? -> char->int "a" str->char char->int z- 10 z+,
    isupper? -> char->int "A" str->char char->int z- 10 z+,
    drop 0
  )

hassign? : Str -- Str Bool
hassign? = dup n1 strbreak drop cond(
  dup "-" streq -> drop true,
  dup "+" streq -> drop true,
  drop false
)
hassign? drop == id

readInt(base: Int) : Str -- Int
readInt(base) =
  hassign? if(
    n1 strbreak readInt(base)
    swap "-" streq if(zneg, id),
    dip(0) strfold(dip(base z*) digitVal z+)
  )

readDecInt : Str -- Int
readHexInt : Str -- Int
readDecInt = readInt(10)
readHexInt = n2 strbreak nip readInt(16)

int->str readDecInt == id

readStr   : Str -- Str
wordToken : Str -- Token  # basically TNAME but separates out reserved words.
wordToken =
  cond(
    dup "="      streq -> drop TEQUAL,
    dup "=="     streq -> drop TEQUAL2,
    dup "--"     streq -> drop TDASH2,
    dup "import" streq -> drop TIMPORT,
    dup "export" streq -> drop TEXPORT,
    dup "type"   streq -> drop TTYPE,
    dup "data"   streq -> drop TDATA,
    dup "end"    streq -> drop TEND,
    TNAME
  )


#
# regexes
#

decIntRegex : Regex
decIntRegex = "" lit "+-" cls alt2 "0123456789" cls plus seq2

hexIntRegex : Regex
hexIntRegex = "" lit "+-" cls alt2 "0" lit "xX" cls "0123456789abcdefABCDEF" cls plus seq4

wordRegex : Regex
wordRegex = "0123456789abcdefghijklmnopqrstuvwxyzzABCDEFGHIJKLMNOPQRSTUVWXYZ_-?+*~!@$%^&=/|<>'" cls plus

trimRegex : Regex
trimRegex = " \t\r" cls star    # need better cls support for comments.


#
#
#

data LexerRule
  lexerRule(f: Str -- Token) : Regex -- LexerRule
end


lexerRules : List(LexerRule)
lexerRules = $(
  nil

  # literals
  "(" lit lexerRule(drop TLPAREN) cons
  ")" lit lexerRule(drop TRPAREN) cons
  "," lit lexerRule(drop TCOMMA) cons
  ":" lit lexerRule(drop TCOLON) cons

  # regexes
  decIntRegex lexerRule(readDecInt TINT) cons
  hexIntRegex lexerRule(readHexInt TINT) cons
  wordRegex   lexerRule(wordToken)       cons

  # newlines
  "\n" lit trimRegex "\n" lit seq2 star seq2 lexerRule(drop TNEWLINE) cons

  # strings
  # TODO
)

data LexerMatch
  lexerMatch(f: Str -- Token) : Maybe(Nat) -- LexerMatch
end

lexerMatchUnwrap : LexerMatch -- Maybe(Nat)
lexerMatchUnwrap = match( lexerMatch(f) -> id )

lexerMatchCmp? : LexerMatch LexerMatch -- LexerMatch LexerMatch Comp
lexerMatchCmp? = dup2 both(lexerMatchUnwrap) mcmp(ncmp)

lexerMatchOrder : Order(LexerMatch)
lexerMatchOrder = MkOrder(lexerMatchCmp?)

bestLexerMatch : Str -- LexerMatch
bestLexerMatch =
  lexerRules formap(
    match( lexerRule(f) ->
      dip(dup) regexMatch lexerMatch(f)
    )
  ) nip maximum(lexerMatchOrder)

data LexerError
  lexerError : Loc Str -- LexerError
end

unLexerError : LexerError -- Loc Str
unLexerError = match( lexerError -> id )

lexerError->str : LexerError -- Str
lexerError->str = unLexerError dip(loc->str ":" <>) <>

data LToken
  ltoken : Loc Loc Token -- LToken
end

unltoken : LToken -- Loc Loc Token
unltoken = match( ltoken -> id )

trim : Loc Str -- Loc Str
trim = dup trimRegex regexMatch maybe(n0, id) strbreak dip(locNext)

tokenizeAux : List(LToken) Loc Str -- Result(LexerError, List(LToken))
tokenizeAux =
  trim cond(
    strnull? -> drop2 ok,
    dup bestLexerMatch match( lexerMatch(f) ->
      maybe(
        drop nip "Unrecognized token." lexerError err,
        strbreak dip(
          dip(dup) dup
          dip(locNext) f over
          dip(ltoken cons)
        ) tokenizeAux
      )
    )
  )

tokenize : Mod Str -- Result(LexerError, List(LToken))
tokenize = dip2(nil) dip(locStart) tokenizeAux

"10" tokenize
  == lambda(m -> nil m p1 p1 loc m p1 3 zpos loc 10 TINT ltoken cons ok)

"+42" tokenize
  == lambda(m -> nil m p1 p1 loc m p1 4 zpos loc +42 TINT ltoken cons ok)

"-5133" tokenize
  == lambda(m -> nil m p1 p1 loc m p1 6 zpos loc -5133 TINT ltoken cons ok)

"0" tokenize
  == lambda(m -> nil m p1 p1 loc m p1 2 zpos loc 0 TINT ltoken cons ok)

"0xFF" tokenize
  == lambda(m -> nil m p1 p1 loc m p1 5 zpos loc 255 TINT ltoken cons ok)

"  10" tokenize
  == lambda(m -> nil m p1 3 zpos loc m p1 5 zpos loc 10 TINT ltoken cons ok)

"" tokenize == drop nil ok
"      " tokenize == drop nil ok
"  \t  \r   " tokenize == drop nil ok


"foobar" tokenize
  == lambda(m -> nil m p1 p1 loc m p1 7 zpos loc "foobar" TNAME ltoken cons ok)


